{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Assignment Video Presentation: https://drive.google.com/file/d/1Ne4azMrF_ScRcSIzIkMrWnlUy0emc3BV/view?usp=sharing\n",
    "\n",
    "### Extra video going over this Spark notebook codes (only codes, no explanation of the project): https://drive.google.com/file/d/1ebKJJSnGheQXzn1d95VlESo4LdL0K5XP/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is pulled from: https://www.kaggle.com/jillwang87/online-retail-ii?select=online_retail_10_11.csv which was originally published on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "\n",
    "\"Data Set Information:\n",
    "\n",
    "This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2010 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "InvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n",
    "StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
    "Description: Product (item) name. Nominal.\n",
    "Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n",
    "UnitPrice: Unit price. Numeric. Product price per unit in sterling (Â£).\n",
    "CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n",
    "Country: Country name. Nominal. The name of the country where a customer resides.\" \n",
    "                                                                               (from UCI Machine Learning Repository)\n",
    "                                                                               \n",
    "This notebook will analyze the data within the Online Retail II data set and answers some relating business questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Spark environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql.functions import when, count, col, countDistinct, desc, first, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Read DataFrame from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .csv(\"online_retail_10_11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |22752    |SET 7 BABUSHKA NESTING BOXES       |2       |12/1/2010 8:26|7.65     |17850     |United Kingdom|\n",
      "|536365   |21730    |GLASS STAR FROSTED T-LIGHT HOLDER  |6       |12/1/2010 8:26|4.25     |17850     |United Kingdom|\n",
      "|536366   |22633    |HAND WARMER UNION JACK             |6       |12/1/2010 8:28|1.85     |17850     |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT          |6       |12/1/2010 8:28|1.85     |17850     |United Kingdom|\n",
      "|536368   |22960    |JAM MAKING SET WITH JARS           |6       |12/1/2010 8:34|4.25     |13047     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a general idea of what the data frame looks like\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Data set metadata analysis\n",
    "Display schema and size of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541910"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many rows the data frame has\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the Schema of the data frame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Data cleaning and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are null values for CustomerID, so we need to filter out those customers that do not have IDs\n",
    "For the description, if it is null, it would be impossible for us to analyze what exactly they bought. So for the purpose our analysis, we take out rows without descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|CustomerId|Description|\n",
      "+----------+-----------+\n",
      "|    135080|       1454|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls in CustomerId and Description\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"CustomerId\", \"Description\"]]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536368|    22960|JAM MAKING SET WI...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536368|    22913|RED COAT RACK PAR...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536368|    22912|YELLOW COAT RACK ...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536368|    22914|BLUE COAT RACK PA...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting rid of the rows with nulls for CustomerId and Description\n",
    "df = df.filter(df[\"CustomerId\"].isNotNull())\\\n",
    "    .filter(df[\"Description\"].isNotNull())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will work on the column InvoiceDate. Right now it is a string type and we will convert it into date format, extract year, month, and day, and get day of the week of the invoice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|DateInvoice|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|  12/1/2010|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|  12/1/2010|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|  12/1/2010|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|  12/1/2010|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|  12/1/2010|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|     7.65|     17850|United Kingdom|  12/1/2010|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|     4.25|     17850|United Kingdom|  12/1/2010|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|     1.85|     17850|United Kingdom|  12/1/2010|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|     1.85|     17850|United Kingdom|  12/1/2010|\n",
      "|   536368|    22960|JAM MAKING SET WI...|       6|     4.25|     13047|United Kingdom|  12/1/2010|\n",
      "|   536368|    22913|RED COAT RACK PAR...|       3|     4.95|     13047|United Kingdom|  12/1/2010|\n",
      "|   536368|    22912|YELLOW COAT RACK ...|       3|     4.95|     13047|United Kingdom|  12/1/2010|\n",
      "|   536368|    22914|BLUE COAT RACK PA...|       3|     4.95|     13047|United Kingdom|  12/1/2010|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|     1.69|     13047|United Kingdom|  12/1/2010|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|  12/1/2010|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|  12/1/2010|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|     3.75|     13047|United Kingdom|  12/1/2010|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|     1.65|     13047|United Kingdom|  12/1/2010|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|     4.25|     13047|United Kingdom|  12/1/2010|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|     4.95|     13047|United Kingdom|  12/1/2010|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the hours and minutes of InvoiceDate and only keep the dates\n",
    "df = df.withColumn(\"DateInvoice\", split(col(\"InvoiceDate\"), \" \").getItem(0)).drop(\"InvoiceDate\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----------+-----+---+----+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|DateInvoice|Month|Day|Year|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----------+-----+---+----+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|     7.65|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|     4.25|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|     1.85|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|     1.85|     17850|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536368|    22960|JAM MAKING SET WI...|       6|     4.25|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536368|    22913|RED COAT RACK PAR...|       3|     4.95|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536368|    22912|YELLOW COAT RACK ...|       3|     4.95|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536368|    22914|BLUE COAT RACK PA...|       3|     4.95|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|     1.69|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|     3.75|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|     1.65|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|     4.25|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|     4.95|     13047|United Kingdom|  12/1/2010|   12|  1|2010|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----------+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dates in the DateInvoice column and assign corresponding values to three new columns: Month, Day, and Year\n",
    "df = df.withColumn(\"Month\", split(col(\"DateInvoice\"), \"/\").getItem(0))\\\n",
    "               .withColumn(\"Day\", split(col(\"DateInvoice\"), \"/\").getItem(1))\\\n",
    "               .withColumn(\"Year\", split(col(\"DateInvoice\"), \"/\").getItem(2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----+---+----+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|Month|Day|Year|InvoiceDate|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----+---+----+-----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|     7.65|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|     4.25|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|     1.85|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|     1.85|     17850|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536368|    22960|JAM MAKING SET WI...|       6|     4.25|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536368|    22913|RED COAT RACK PAR...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536368|    22912|YELLOW COAT RACK ...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536368|    22914|BLUE COAT RACK PA...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|     1.69|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|     3.75|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|     1.65|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|     4.25|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----+---+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here I concactinate columns Year, Month, and Day together and cast it as date type. \n",
    "# I name this column InvoiceDate and drop the DateInvoice column which was of string type\n",
    "df = df.withColumn(\"InvoiceDate\",concat_ws(\"-\",col(\"Year\"),col(\"Month\"),col(\"Day\")).cast(\"date\"))\\\n",
    "        .drop(\"DateInvoice\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----+---+----+-----------+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|UnitPrice|CustomerID|       Country|Month|Day|Year|InvoiceDate|DayofWeek|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----+---+----+-----------+---------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|     2.55|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|     3.39|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|     2.75|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|     3.39|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|     3.39|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|     7.65|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|     4.25|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|     1.85|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|     1.85|     17850|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536368|    22960|JAM MAKING SET WI...|       6|     4.25|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536368|    22913|RED COAT RACK PAR...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536368|    22912|YELLOW COAT RACK ...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536368|    22914|BLUE COAT RACK PA...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|     1.69|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|      2.1|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|     3.75|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|     1.65|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|     4.25|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|     4.95|     13047|United Kingdom|   12|  1|2010| 2010-12-01|      Wed|\n",
      "+---------+---------+--------------------+--------+---------+----------+--------------+-----+---+----+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column DayofWeek to see the day of the week of the corresponding invoice date\n",
    "df = df.withColumn(\"DayofWeek\", date_format(col(\"InvoiceDate\"), \"E\")).cache() #cache df to make futher analysis faster\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- DayofWeek: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() #verify we did the transformation successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Columns groups basic profiling and overview questions\n",
    "## Timing related section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of columns Year, Month, Day, DayofWeek\n",
      "+-------+-------------------+-----------------+------------------+---------+\n",
      "|summary|               Year|            Month|               Day|DayofWeek|\n",
      "+-------+-------------------+-----------------+------------------+---------+\n",
      "|  count|             406830|           406830|            406830|   406830|\n",
      "|   mean| 2010.9340019172628|7.605958262664995|15.036113364304502|     null|\n",
      "| stddev|0.24827905132967706|3.418945197059456| 8.653724410199423|     null|\n",
      "|    min|               2010|                1|                 1|      Fri|\n",
      "|    25%|             2011.0|              5.0|               7.0|     null|\n",
      "|    50%|             2011.0|              8.0|              15.0|     null|\n",
      "|    75%|             2011.0|             11.0|              22.0|     null|\n",
      "|    max|               2011|                9|                 9|      Wed|\n",
      "+-------+-------------------+-----------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Summary of columns Year, Month, Day, DayofWeek\")\n",
    "df.select(\"Year\", \"Month\", \"Day\", \"DayofWeek\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for nulls on columns Year, Month, DayofMonth and DayOfWeek:\n",
      "+----+-----+---+---------+\n",
      "|Year|Month|Day|DayofWeek|\n",
      "+----+-----+---+---------+\n",
      "|   0|    0|  0|        0|\n",
      "+----+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for nulls on columns Year, Month, DayofMonth and DayOfWeek:\")\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"Year\", \"Month\", \"Day\", \"DayofWeek\"]]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking amount of distinct values in columns Year, Month, DayofMonth and DayOfWeek:\n",
      "+----+-----+---+---------+\n",
      "|Year|Month|Day|DayofWeek|\n",
      "+----+-----+---+---------+\n",
      "|   2|   12| 31|        6|\n",
      "+----+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking amount of distinct values in columns Year, Month, DayofMonth and DayOfWeek:\")\n",
    "df.select([countDistinct(c).alias(c) for c in [\"Year\", \"Month\", \"Day\", \"DayofWeek\"]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the data covers from December 2010 to December 2011 so it is not surprising to see the distinct values for year is 2 (2010 & 2011), month to be 12, and day to be 31. However,there are only 6 days of the week distinct values so let's investigate further and answer the following questions:\n",
    "\n",
    "#### 1. What are the most and least frequent occurrences for DayofWeek column?\n",
    "#### 2. What are the most and least frequent occurrences for Month column?\n",
    "#### 3. What are the most frequent occurrences for Day column?\n",
    "#### 4. What are the most frequent occurrences for InvoiceDate column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most and least frequent occurrences for DayofWeek column:\n",
      "+---------+-----+\n",
      "|DayofWeek|count|\n",
      "+---------+-----+\n",
      "|      Thu|82374|\n",
      "|      Wed|70599|\n",
      "|      Tue|68110|\n",
      "|      Mon|66382|\n",
      "|      Sun|63237|\n",
      "|      Fri|56128|\n",
      "+---------+-----+\n",
      "\n",
      "Most and least frequent occurrences for Month column:\n",
      "+-----+-----+\n",
      "|Month|count|\n",
      "+-----+-----+\n",
      "|   11|65598|\n",
      "|   10|50695|\n",
      "|   12|44512|\n",
      "|    9|40822|\n",
      "|    5|28908|\n",
      "|    6|27836|\n",
      "|    3|27822|\n",
      "|    8|27662|\n",
      "|    7|27502|\n",
      "|    4|23198|\n",
      "|    1|21912|\n",
      "|    2|20363|\n",
      "+-----+-----+\n",
      "\n",
      "Most frequent occurrences for Day column:\n",
      "+---+-----+\n",
      "|Day|count|\n",
      "+---+-----+\n",
      "|  6|18915|\n",
      "|  5|16723|\n",
      "|  8|16254|\n",
      "|  7|16011|\n",
      "|  4|15165|\n",
      "| 17|15139|\n",
      "| 20|14940|\n",
      "| 14|14549|\n",
      "| 23|14545|\n",
      "| 10|14473|\n",
      "| 13|14401|\n",
      "|  1|13937|\n",
      "| 28|13778|\n",
      "| 11|13707|\n",
      "| 21|13509|\n",
      "| 18|13234|\n",
      "|  9|13226|\n",
      "| 16|12910|\n",
      "| 27|12708|\n",
      "| 22|12583|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Most frequent occurrences for InvoiceDate column:\n",
      "+-----------+-----+\n",
      "|InvoiceDate|count|\n",
      "+-----------+-----+\n",
      "| 2011-11-06| 3434|\n",
      "| 2011-12-05| 3398|\n",
      "| 2011-11-23| 3332|\n",
      "| 2011-11-10| 3194|\n",
      "| 2011-11-20| 3112|\n",
      "| 2011-11-17| 3034|\n",
      "| 2011-11-14| 2964|\n",
      "| 2011-10-30| 2907|\n",
      "| 2011-10-06| 2870|\n",
      "| 2011-11-22| 2862|\n",
      "| 2011-11-28| 2789|\n",
      "| 2011-10-10| 2783|\n",
      "| 2011-11-16| 2772|\n",
      "| 2011-11-13| 2757|\n",
      "| 2011-11-29| 2736|\n",
      "| 2010-12-05| 2724|\n",
      "| 2011-12-06| 2654|\n",
      "| 2011-12-08| 2599|\n",
      "| 2011-11-04| 2571|\n",
      "| 2011-09-22| 2505|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Most and least frequent occurrences for DayofWeek column:\")\n",
    "df.groupBy(\"DayofWeek\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "print (\"Most and least frequent occurrences for Month column:\")\n",
    "df.groupBy(\"Month\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "print (\"Most frequent occurrences for Day column:\")\n",
    "df.groupBy(\"Day\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "print (\"Most frequent occurrences for InvoiceDate column:\")\n",
    "df.groupBy(\"InvoiceDate\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we group by DayofWeek, we see that this online retail store do not accept invoices on Saturday, which is a unique choice.\n",
    "We also see that the most frequent occurrances for month is November, for day is the 6th, and on November 6th, 2011, the most number of invoices were places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item and geographical location related section\n",
    "#### 5. What are the top 10 countries with the most customers?\n",
    "Every customer has an unique CustomerID, so the total number of customers is count(distinct CustomerID). Then we group by country and order by count of customers in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists df1\")\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|       Country|countCustomer|\n",
      "+--------------+-------------+\n",
      "|United Kingdom|         3950|\n",
      "|       Germany|           95|\n",
      "|        France|           87|\n",
      "|         Spain|           31|\n",
      "|       Belgium|           25|\n",
      "|   Switzerland|           21|\n",
      "|      Portugal|           19|\n",
      "|         Italy|           15|\n",
      "|       Finland|           12|\n",
      "|       Austria|           11|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select Country,\n",
    "                count(distinct CustomerID) as countCustomer\n",
    "            from df1\n",
    "            group by Country\n",
    "            order by countCustomer desc\n",
    "            limit 10\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What are the top 10 most sold items?\n",
    "Using sum(Quantity) to calculate total sales and group by StockCode to calculate total sales by item. \n",
    "We find that 84077, which is \"WORLD WAR 2 GLIDERS ASSTD DESIGNS\" is the most sold item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|StockCode|sumQuantity|\n",
      "+---------+-----------+\n",
      "|    84077|      53215|\n",
      "|    22197|      48712|\n",
      "|   85099B|      45066|\n",
      "|    84879|      35314|\n",
      "|   85123A|      34204|\n",
      "|    21212|      33409|\n",
      "|    23084|      27094|\n",
      "|    22492|      25880|\n",
      "|    22616|      25321|\n",
      "|    21977|      24163|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select StockCode,\n",
    "                sum(Quantity) as sumQuantity\n",
    "            from df1\n",
    "            group by StockCode\n",
    "            order by sumQuantity desc\n",
    "            limit 10\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------+\n",
      "|StockCode|Description                      |\n",
      "+---------+---------------------------------+\n",
      "|84077    |WORLD WAR 2 GLIDERS ASSTD DESIGNS|\n",
      "+---------+---------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select StockCode, Description\n",
    "            from df1\n",
    "            where StockCode = 84077\"\"\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What are the top 10 countries by quantity of items sold?\n",
    "Under the Quantity column, returned items are marked as negatives. Therefore, by using sum(Quantity) we can calculate total sales taken into account returned items. Then we group by countries and order by the sum of quantity in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|       country|sumQuantity|\n",
      "+--------------+-----------+\n",
      "|United Kingdom|    4008533|\n",
      "|   Netherlands|     200128|\n",
      "|          EIRE|     136329|\n",
      "|       Germany|     117448|\n",
      "|        France|     109849|\n",
      "|     Australia|      83653|\n",
      "|        Sweden|      35637|\n",
      "|   Switzerland|      29778|\n",
      "|         Spain|      26824|\n",
      "|         Japan|      25218|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select country,\n",
    "              sum(quantity) as sumQuantity\n",
    "          from df1\n",
    "          group by country\n",
    "          order by sumQuantity desc\n",
    "          limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. How are the sales figure like for each country?\n",
    "By using sum(UnitPrice * Quantity), we can calcualte total sales in pounds(£). Then we group by country, order by sum of sales in descending order, and see that in UK the retail store achieved the largest sales figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|        country|          sumSales|\n",
      "+---------------+------------------+\n",
      "| United Kingdom|  6767873.39400001|\n",
      "|    Netherlands| 284661.5399999992|\n",
      "|           EIRE|250285.21999999924|\n",
      "|        Germany|221698.21000000037|\n",
      "|         France|196730.84000000043|\n",
      "|      Australia|137077.26999999987|\n",
      "|    Switzerland| 55739.40000000008|\n",
      "|          Spain| 54774.58000000016|\n",
      "|        Belgium|40910.960000000014|\n",
      "|         Sweden| 36595.90999999998|\n",
      "|          Japan|35340.619999999995|\n",
      "|         Norway| 35163.46000000001|\n",
      "|       Portugal|29059.809999999954|\n",
      "|        Finland|22326.739999999994|\n",
      "|Channel Islands| 20086.28999999999|\n",
      "|        Denmark| 18768.13999999999|\n",
      "|          Italy|16890.509999999995|\n",
      "|         Cyprus|12946.289999999994|\n",
      "|        Austria|10154.319999999996|\n",
      "|      Singapore| 9120.390000000001|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select country,\n",
    "                sum(UnitPrice * Quantity) as sumSales\n",
    "            from df1\n",
    "            group by country\n",
    "            order by sumSales desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What are the top 10 countries with the most return orders?\n",
    "We can use count(distinct InvoiceNo) to calculate the total number of orders and since the returned order begins with \"C\" rather than integer, we can use wildcard like 'C%' to filter out those returned orders. Then we group by country and see that UK has the most return orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|       country|sumReturn|\n",
      "+--------------+---------+\n",
      "|United Kingdom|     3208|\n",
      "|       Germany|      146|\n",
      "|        France|       69|\n",
      "|          EIRE|       59|\n",
      "|       Belgium|       21|\n",
      "|   Switzerland|       20|\n",
      "|         Italy|       17|\n",
      "|         Spain|       15|\n",
      "|      Portugal|       13|\n",
      "|     Australia|       12|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select country,\n",
    "                count(distinct InvoiceNo) as sumReturn\n",
    "            from df1\n",
    "            where InvoiceNo like 'C%'\n",
    "            group by country\n",
    "            order by sumReturn desc\n",
    "            limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. More business questions\n",
    "\n",
    "## 10. What are the top key words?\n",
    "First, we have to prepare the data we need and then we prepare the schema of the dataframe. Finally, we make the word_count_df that sorts words in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation section:\n",
    "Step 1: Select & change the descriptions to lower case so when we count them later, same word with different capitalization will not be counted multiple times\n",
    "\n",
    "Step 2: Transform step 1 into a resilient distributed dataset (RDD) and use flatMap to Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. The function we applied here is to split each row of description by space so we extract the individual words\n",
    "\n",
    "Step 3: Then we use the map transformation to apply a function to every element of the previous RDD (individual words) and return a new RDD. The function we applied here is to create a tuple that appends an integer 1 after each word. The integer 1 could later be used to count.\n",
    "\n",
    "Step 4: Use reduceByKey we sum all the integer values for each unique key\n",
    "\n",
    "Step 5: We sort by the count values in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = spark.sql(\"select lower(description) as words from df1\")\\\n",
    "    .rdd.flatMap(lambda row: row[\"words\"].split(\" \"))\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortBy(lambda x:x[1], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frame creation section:\n",
    "1. Create schema\n",
    "    For the two columns that we will have in our table, \"Word\" will be of StringType and \"Count\" will be of  \n",
    "    IntegerType\n",
    "2. Create DataFrame\n",
    "    First create DF and pass in \"word_count\" as data and \"schema\" as schema\n",
    "    Afterwards, we have to filter out the row where the word is '', since it has the highest count and it is not a  \n",
    "    word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Word|Count|\n",
      "+----------+-----+\n",
      "|       set|41623|\n",
      "|        of|41479|\n",
      "|       bag|38452|\n",
      "|       red|32590|\n",
      "|     heart|29443|\n",
      "| retrospot|27029|\n",
      "|   vintage|26043|\n",
      "|    design|24007|\n",
      "|      pink|20521|\n",
      "| christmas|19334|\n",
      "|       box|18359|\n",
      "|      cake|16630|\n",
      "|     white|16360|\n",
      "|     metal|15897|\n",
      "|     jumbo|15737|\n",
      "|     lunch|15211|\n",
      "|         3|15040|\n",
      "|      blue|14047|\n",
      "|   hanging|13220|\n",
      "|    holder|12975|\n",
      "|      sign|12722|\n",
      "|      pack|12144|\n",
      "|   t-light|11531|\n",
      "|     paper|10845|\n",
      "|     small|10592|\n",
      "|    wooden|10183|\n",
      "|         6| 9782|\n",
      "|     cases| 9655|\n",
      "|      card| 9621|\n",
      "|     glass| 8997|\n",
      "|        12| 8958|\n",
      "|       tea| 8957|\n",
      "|  polkadot| 8937|\n",
      "|decoration| 8792|\n",
      "|  spaceboy| 8713|\n",
      "|    bottle| 8703|\n",
      "|        in| 8563|\n",
      "|       and| 8018|\n",
      "|       hot| 8005|\n",
      "|      home| 8001|\n",
      "|    pantry| 7760|\n",
      "|     large| 7691|\n",
      "|       tin| 7640|\n",
      "|     water| 7625|\n",
      "|   regency| 7195|\n",
      "|      with| 7145|\n",
      "|   ceramic| 7109|\n",
      "|         4| 6886|\n",
      "|   doormat| 6847|\n",
      "|   paisley| 6711|\n",
      "|     dolly| 6699|\n",
      "|     ivory| 6677|\n",
      "|     cream| 6564|\n",
      "|   bunting| 6346|\n",
      "|      rose| 6329|\n",
      "|     green| 6305|\n",
      "|       mug| 6304|\n",
      "| feltcraft| 6298|\n",
      "|      wrap| 6247|\n",
      "|      girl| 6206|\n",
      "|      mini| 6113|\n",
      "|  assorted| 6065|\n",
      "|     clock| 6059|\n",
      "|      love| 5970|\n",
      "|    wicker| 5867|\n",
      "|     party| 5864|\n",
      "|      tins| 5613|\n",
      "|       kit| 5606|\n",
      "|     frame| 5591|\n",
      "|   antique| 5256|\n",
      "|    drawer| 5225|\n",
      "|     black| 5202|\n",
      "|     fairy| 5182|\n",
      "|    colour| 5144|\n",
      "|    garden| 5114|\n",
      "|  woodland| 5050|\n",
      "|    silver| 4857|\n",
      "| childrens| 4845|\n",
      "|      bowl| 4816|\n",
      "|      wall| 4751|\n",
      "|        60| 4722|\n",
      "|      star| 4697|\n",
      "|      wood| 4689|\n",
      "| charlotte| 4532|\n",
      "|  birthday| 4453|\n",
      "|      bird| 4416|\n",
      "|strawberry| 4279|\n",
      "|     union| 4244|\n",
      "|      hand| 4229|\n",
      "|    hearts| 4082|\n",
      "|      zinc| 4072|\n",
      "|      50's| 3995|\n",
      "|       jar| 3978|\n",
      "|       jam| 3972|\n",
      "|      gift| 3939|\n",
      "|         2| 3930|\n",
      "|      knob| 3890|\n",
      "|   cutlery| 3872|\n",
      "|   candles| 3817|\n",
      "|   pencils| 3774|\n",
      "+----------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create schema\n",
    "schema = StructType([\n",
    "    StructField(\"Word\",StringType(),True),\n",
    "    StructField(\"Count\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "# Create DF\n",
    "word_count_df = spark.createDataFrame(word_count, schema) # Create DF\n",
    "word_count_df = word_count_df.filter(word_count_df[\"Word\"] != '') # Filter DF\n",
    "word_count_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regarding monthly data\n",
    "\n",
    "## 11. What are the total sales by month in British Pounds?\n",
    "To get the monthly sales in British Pounds, we need the invoice date, quantity, and unit price. \n",
    "First we create the monthly sales in British Pounds data\n",
    "    Step 1: Create a new column in the original data frame called \"InvoiceMonth\" by concatenating \"Year\" and \"Month\" \n",
    "    with \"-\" and cast that to date type. So the first day of every month will represent the entire month \n",
    "    Step 2: Select \"InvoiceMonth\", \"Quantity\", and \"UnitPrice\" column and transform it into a RDD\n",
    "    Step 3: Use map to transform 3 columns to 2 columns by multiplying \"Quantity\" with \"UnitPrice\" to get total sales \n",
    "    in Pounds\n",
    "    Step 4: Sum the integer values of each unique key using reduceByKey\n",
    "    Step 5: Sort it by Month\n",
    "Second we create the schema for the two column data table: \"Month\" with date type and \"Total_Sales\" with double type\n",
    "Third we create the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|     Month|       Total_Sales|\n",
      "+----------+------------------+\n",
      "|2010-12-01|  554604.020000018|\n",
      "|2011-01-01|475074.38000001636|\n",
      "|2011-02-01| 436546.1500000147|\n",
      "|2011-03-01| 579964.6100000151|\n",
      "|2011-04-01| 426047.8510000125|\n",
      "|2011-05-01|  648251.080000003|\n",
      "|2011-06-01| 608013.1600000106|\n",
      "|2011-07-01|  574238.481000012|\n",
      "|2011-08-01| 616368.0000000028|\n",
      "|2011-09-01| 931440.3719999959|\n",
      "|2011-10-01| 974603.5899999909|\n",
      "|2011-11-01|1132407.7399999578|\n",
      "|2011-12-01| 342524.3800000034|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare monthly sales in British Pounds data\n",
    "sales_m = df.withColumn(\"InvoiceMonth\",concat_ws(\"-\",col(\"Year\"),col(\"Month\")).cast(\"date\"))\\\n",
    "    .select(\"InvoiceMonth\", \"Quantity\", \"UnitPrice\").rdd\\\n",
    "    .map(lambda row:(row[0],row[1]*row[2]))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortByKey()\n",
    "\n",
    "# Create schema\n",
    "schema_m = StructType([\n",
    "    StructField(\"Month\",DateType(),True),\n",
    "    StructField(\"Total_Sales\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "# Create DF\n",
    "monthly_sales_pounds = spark.createDataFrame(sales_m, schema_m) # Create DF\n",
    "monthly_sales_pounds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. What are the total sales by month in quantities?\n",
    "This is similar to previous step except when we prepare the monthly sales in quantity data, we do not need the \"UnitPrice\" column. Then we create the schema and create the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|     Month|Total_Quantity|\n",
      "+----------+--------------+\n",
      "|2010-12-01|        296362|\n",
      "|2011-01-01|        269379|\n",
      "|2011-02-01|        262833|\n",
      "|2011-03-01|        344012|\n",
      "|2011-04-01|        278585|\n",
      "|2011-05-01|        367852|\n",
      "|2011-06-01|        356922|\n",
      "|2011-07-01|        363418|\n",
      "|2011-08-01|        386612|\n",
      "|2011-09-01|        537496|\n",
      "|2011-10-01|        569666|\n",
      "|2011-11-01|        669915|\n",
      "|2011-12-01|        203837|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_m_q = df.withColumn(\"InvoiceMonth\",concat_ws(\"-\",col(\"Year\"),col(\"Month\")).cast(\"date\"))\\\n",
    "    .select(\"InvoiceMonth\", \"Quantity\").rdd\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortByKey()\n",
    "\n",
    "# Create schema\n",
    "schema_m_q = StructType([\n",
    "    StructField(\"Month\",DateType(),True),\n",
    "    StructField(\"Total_Quantity\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "# Create DF\n",
    "monthly_sales_quan = spark.createDataFrame(sales_m_q, schema_m_q) # Create DF\n",
    "monthly_sales_quan.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two data frames to get a more comprehensive view of the monthly sales situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+\n",
      "|     Month|       Total_Sales|Total_Quantity|\n",
      "+----------+------------------+--------------+\n",
      "|2010-12-01|  554604.020000018|        296362|\n",
      "|2011-01-01|475074.38000001636|        269379|\n",
      "|2011-02-01| 436546.1500000147|        262833|\n",
      "|2011-03-01| 579964.6100000151|        344012|\n",
      "|2011-04-01| 426047.8510000125|        278585|\n",
      "|2011-05-01|  648251.080000003|        367852|\n",
      "|2011-06-01| 608013.1600000106|        356922|\n",
      "|2011-07-01|  574238.481000012|        363418|\n",
      "|2011-08-01| 616368.0000000028|        386612|\n",
      "|2011-09-01| 931440.3719999959|        537496|\n",
      "|2011-10-01| 974603.5899999909|        569666|\n",
      "|2011-11-01|1132407.7399999578|        669915|\n",
      "|2011-12-01| 342524.3800000034|        203837|\n",
      "+----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthly_sales = monthly_sales_pounds.join(monthly_sales_quan, \"Month\").sort(\"Month\")\n",
    "monthly_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales.toPandas().to_csv('monthly_sales.csv') #save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regarding daily data\n",
    "\n",
    "## 13. What are the total sales by day in British Pounds?\n",
    "This is similar to the question that asks for monthly sales in British Pounds. Except for here, we do not need to concact Year and Month together as the transformation of \"InvoiceDate\" in the beginning of the notebook has already transformed the column to date type and it's unit is to days rather than to months as in question 11.\n",
    "The rest of the steps are the same as to question 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Date|       Total_Sales|\n",
      "+----------+------------------+\n",
      "|2010-12-01| 46051.26000000007|\n",
      "|2010-12-02|  45775.4299999999|\n",
      "|2010-12-03|22598.460000000086|\n",
      "|2010-12-05|31380.600000000162|\n",
      "|2010-12-06|30465.080000000165|\n",
      "|2010-12-07| 53125.99000000011|\n",
      "|2010-12-08|38048.680000000095|\n",
      "|2010-12-09|37177.850000000035|\n",
      "|2010-12-10| 32005.35000000008|\n",
      "|2010-12-12| 17217.62000000005|\n",
      "|2010-12-13|27429.430000000066|\n",
      "|2010-12-14| 26913.41000000016|\n",
      "|2010-12-15| 29310.79000000012|\n",
      "|2010-12-16| 48011.52999999994|\n",
      "|2010-12-17|18162.420000000035|\n",
      "|2010-12-19|  7399.78999999999|\n",
      "|2010-12-20| 17574.58000000003|\n",
      "|2010-12-21|15750.460000000005|\n",
      "|2010-12-22| 4821.119999999998|\n",
      "|2010-12-23| 5384.169999999993|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_d = df.select(\"InvoiceDate\", \"Quantity\", \"UnitPrice\").rdd\\\n",
    "    .map(lambda row: (row[0], row[1]*row[2]))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortByKey()\n",
    "\n",
    "# Create schema\n",
    "schema_d = StructType([\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Total_Sales\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "# Create DF\n",
    "daily_sales_pounds = spark.createDataFrame(sales_d, schema_d) # Create DF\n",
    "daily_sales_pounds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. What are the total sales by day in quantities?\n",
    "This is similar to question 12, except for here we change everything from monthly to daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      Date|Total_Quantity|\n",
      "+----------+--------------+\n",
      "|2010-12-01|         24032|\n",
      "|2010-12-02|         20855|\n",
      "|2010-12-03|         11548|\n",
      "|2010-12-05|         16394|\n",
      "|2010-12-06|         16095|\n",
      "|2010-12-07|         19351|\n",
      "|2010-12-08|         21275|\n",
      "|2010-12-09|         16904|\n",
      "|2010-12-10|         15388|\n",
      "|2010-12-12|         10561|\n",
      "|2010-12-13|         15234|\n",
      "|2010-12-14|         17108|\n",
      "|2010-12-15|         18169|\n",
      "|2010-12-16|         29482|\n",
      "|2010-12-17|         10517|\n",
      "|2010-12-19|          3735|\n",
      "|2010-12-20|         12617|\n",
      "|2010-12-21|         10888|\n",
      "|2010-12-22|          3053|\n",
      "|2010-12-23|          3156|\n",
      "+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_d_q = df.select(\"InvoiceDate\", \"Quantity\").rdd\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortByKey()\n",
    "\n",
    "# Create schema\n",
    "schema_d_q = StructType([\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Total_Quantity\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "# Create DF\n",
    "daily_sales_quan = spark.createDataFrame(sales_d_q, schema_d_q) # Create DF\n",
    "daily_sales_quan.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two data frames together to get a more comprehensive view of the daily sales situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+\n",
      "|      Date|       Total_Sales|Total_Quantity|\n",
      "+----------+------------------+--------------+\n",
      "|2010-12-01| 46051.26000000007|         24032|\n",
      "|2010-12-02|  45775.4299999999|         20855|\n",
      "|2010-12-03|22598.460000000086|         11548|\n",
      "|2010-12-05|31380.600000000162|         16394|\n",
      "|2010-12-06|30465.080000000165|         16095|\n",
      "|2010-12-07| 53125.99000000011|         19351|\n",
      "|2010-12-08|38048.680000000095|         21275|\n",
      "|2010-12-09|37177.850000000035|         16904|\n",
      "|2010-12-10| 32005.35000000008|         15388|\n",
      "|2010-12-12| 17217.62000000005|         10561|\n",
      "|2010-12-13|27429.430000000066|         15234|\n",
      "|2010-12-14| 26913.41000000016|         17108|\n",
      "|2010-12-15| 29310.79000000012|         18169|\n",
      "|2010-12-16| 48011.52999999994|         29482|\n",
      "|2010-12-17|18162.420000000035|         10517|\n",
      "|2010-12-19|  7399.78999999999|          3735|\n",
      "|2010-12-20| 17574.58000000003|         12617|\n",
      "|2010-12-21|15750.460000000005|         10888|\n",
      "|2010-12-22| 4821.119999999998|          3053|\n",
      "|2010-12-23| 5384.169999999993|          3156|\n",
      "+----------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_sales = daily_sales_pounds.join(daily_sales_quan, \"Date\").sort(\"Date\")\n",
    "daily_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales.toPandas().to_csv('daily_sales.csv') #save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Situation\n",
    "## 15. Which country has the most amount of return invoices?\n",
    "Step 1: create a returndf by using the code from question 9\n",
    "Step 2: create a buydf by using similar logic but here the wild card is not like 'C%' so we get InvoiceNo without \"C\"\n",
    "    in the beginning to indicate that they were not returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return DF\n",
      "+------------------+---------+\n",
      "|           Country|sumReturn|\n",
      "+------------------+---------+\n",
      "|            Sweden|       10|\n",
      "|         Singapore|        3|\n",
      "|           Germany|      146|\n",
      "|            France|       69|\n",
      "|            Greece|        1|\n",
      "|European Community|        1|\n",
      "|           Belgium|       21|\n",
      "|           Finland|        7|\n",
      "|             Malta|        5|\n",
      "|             Italy|       17|\n",
      "|              EIRE|       59|\n",
      "|            Norway|        4|\n",
      "|             Spain|       15|\n",
      "|           Denmark|        3|\n",
      "|            Israel|        1|\n",
      "|   Channel Islands|        7|\n",
      "|               USA|        2|\n",
      "|            Cyprus|        4|\n",
      "|      Saudi Arabia|        1|\n",
      "|       Switzerland|       20|\n",
      "+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Buy DF\n",
      "+------------------+------+\n",
      "|         country_b|sumBuy|\n",
      "+------------------+------+\n",
      "|            Sweden|    36|\n",
      "|         Singapore|     7|\n",
      "|           Germany|   457|\n",
      "|               RSA|     1|\n",
      "|            France|   389|\n",
      "|            Greece|     5|\n",
      "|European Community|     4|\n",
      "|           Belgium|    98|\n",
      "|           Finland|    41|\n",
      "|             Malta|     5|\n",
      "|       Unspecified|     8|\n",
      "|             Italy|    38|\n",
      "|              EIRE|   260|\n",
      "|         Lithuania|     4|\n",
      "|            Norway|    36|\n",
      "|             Spain|    90|\n",
      "|           Denmark|    18|\n",
      "|           Iceland|     7|\n",
      "|            Israel|     5|\n",
      "|   Channel Islands|    26|\n",
      "+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Return DF\")\n",
    "returndf = spark.sql(\"\"\"select country as Country,\n",
    "                count(distinct InvoiceNo) as sumReturn\n",
    "            from df1\n",
    "            where InvoiceNo like 'C%'\n",
    "            group by Country\"\"\")\n",
    "returndf.show()\n",
    "\n",
    "print(\"Buy DF\")\n",
    "buydf = spark.sql(\"\"\"select country as country_b,\n",
    "              count(distinct InvoiceNo) as sumBuy\n",
    "          from df1\n",
    "          where InvoiceNo not like 'C%'\n",
    "          group by country_b\"\"\")\n",
    "buydf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Join returndf and buydf together. Since we care about the returns, we use left_outer to make sure all \n",
    "    countries with customers that have returned will be captured by our data frame. Since the newly joined \n",
    "    return_ratio_df will have two country columns, we only select one of them and rank them by sum of returns in \n",
    "    descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+---------+\n",
      "|        Country|sumBuy|sumReturn|\n",
      "+---------------+------+---------+\n",
      "| United Kingdom| 16649|     3208|\n",
      "|        Germany|   457|      146|\n",
      "|         France|   389|       69|\n",
      "|           EIRE|   260|       59|\n",
      "|        Belgium|    98|       21|\n",
      "|    Switzerland|    51|       20|\n",
      "|          Italy|    38|       17|\n",
      "|          Spain|    90|       15|\n",
      "|       Portugal|    57|       13|\n",
      "|      Australia|    57|       12|\n",
      "|         Sweden|    36|       10|\n",
      "|          Japan|    19|        9|\n",
      "|Channel Islands|    26|        7|\n",
      "|        Finland|    41|        7|\n",
      "|    Netherlands|    95|        6|\n",
      "|          Malta|     5|        5|\n",
      "|         Poland|    19|        5|\n",
      "|         Cyprus|    16|        4|\n",
      "|         Norway|    36|        4|\n",
      "| Czech Republic|     2|        3|\n",
      "+---------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_ratio_df = returndf.join(buydf, returndf[\"Country\"] == buydf[\"country_b\"], \"left_outer\")\n",
    "return_ratio_df = return_ratio_df.select(return_ratio_df[\"Country\"],\n",
    "                                         return_ratio_df[\"sumBuy\"],return_ratio_df[\"sumReturn\"])\n",
    "return_ratio_df.sort(F.col(\"sumReturn\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Which country has the highest and the lowest return ratios?\n",
    "First have to add a new column \"ReturnRatio\" by computing the ratio between sum of return and sum of purchase.\n",
    "To find the highest return ratio, we rank it in descending order. Here we see anoutlier of Czech Republich where only 2 purchase orders were made but received 3 return orders from December 2010 to December 2011. It is very possible that someone from there purchased an item a couple months before December 2010 and its purchased was not noted within our data frame but the return was.\n",
    "To find the lowest return ratio, we sort it by ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return ratios ranked in descending order\n",
      "+------------------+------+---------+-------------------+\n",
      "|           Country|sumBuy|sumReturn|        ReturnRatio|\n",
      "+------------------+------+---------+-------------------+\n",
      "|    Czech Republic|     2|        3|                1.5|\n",
      "|      Saudi Arabia|     1|        1|                1.0|\n",
      "|             Malta|     5|        5|                1.0|\n",
      "|             Japan|    19|        9|0.47368421052631576|\n",
      "|             Italy|    38|       17| 0.4473684210526316|\n",
      "|         Singapore|     7|        3|0.42857142857142855|\n",
      "|               USA|     5|        2|                0.4|\n",
      "|       Switzerland|    51|       20|0.39215686274509803|\n",
      "|           Germany|   457|      146|0.31947483588621445|\n",
      "|            Sweden|    36|       10| 0.2777777777777778|\n",
      "|   Channel Islands|    26|        7| 0.2692307692307692|\n",
      "|            Poland|    19|        5| 0.2631578947368421|\n",
      "|European Community|     4|        1|               0.25|\n",
      "|            Cyprus|    16|        4|               0.25|\n",
      "|          Portugal|    57|       13|0.22807017543859648|\n",
      "|              EIRE|   260|       59|0.22692307692307692|\n",
      "|           Belgium|    98|       21|0.21428571428571427|\n",
      "|         Australia|    57|       12|0.21052631578947367|\n",
      "|            Greece|     5|        1|                0.2|\n",
      "|            Israel|     5|        1|                0.2|\n",
      "+------------------+------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Return ratios ranked in ascending order\n",
      "+------------------+------+---------+-------------------+\n",
      "|           Country|sumBuy|sumReturn|        ReturnRatio|\n",
      "+------------------+------+---------+-------------------+\n",
      "|       Netherlands|    95|        6|0.06315789473684211|\n",
      "|            Norway|    36|        4| 0.1111111111111111|\n",
      "|           Austria|    17|        2|0.11764705882352941|\n",
      "|             Spain|    90|       15|0.16666666666666666|\n",
      "|           Denmark|    18|        3|0.16666666666666666|\n",
      "|           Finland|    41|        7|0.17073170731707318|\n",
      "|            France|   389|       69|0.17737789203084833|\n",
      "|    United Kingdom| 16649|     3208|0.19268424530001801|\n",
      "|            Greece|     5|        1|                0.2|\n",
      "|            Israel|     5|        1|                0.2|\n",
      "|         Australia|    57|       12|0.21052631578947367|\n",
      "|           Belgium|    98|       21|0.21428571428571427|\n",
      "|              EIRE|   260|       59|0.22692307692307692|\n",
      "|          Portugal|    57|       13|0.22807017543859648|\n",
      "|European Community|     4|        1|               0.25|\n",
      "|            Cyprus|    16|        4|               0.25|\n",
      "|            Poland|    19|        5| 0.2631578947368421|\n",
      "|   Channel Islands|    26|        7| 0.2692307692307692|\n",
      "|            Sweden|    36|       10| 0.2777777777777778|\n",
      "|           Germany|   457|      146|0.31947483588621445|\n",
      "+------------------+------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Return ratios ranked in descending order\")\n",
    "return_ratio_df = return_ratio_df.withColumn(\"ReturnRatio\", return_ratio_df.sumReturn/return_ratio_df.sumBuy)\\\n",
    "    .sort(F.col(\"ReturnRatio\").desc())\n",
    "return_ratio_df.show()\n",
    "\n",
    "print (\"Return ratios ranked in ascending order\")\n",
    "return_ratio_df.sort(F.col(\"ReturnRatio\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio_df.toPandas().to_csv('returnratio.csv') #save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. What is the return ratio situation like for countries with more than 10 invoices?\n",
    "Filter by column \"sumBuy\" and set it to greater than 10. Since return_ratio_df is already sorted by return ratio in descending order, the output will be in that order as well adn we can see Japan has the highest return ratio for more than 10 orders and Netherlands on the contrary has the lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+---------+-------------------+\n",
      "|        Country|sumBuy|sumReturn|        ReturnRatio|\n",
      "+---------------+------+---------+-------------------+\n",
      "|          Japan|    19|        9|0.47368421052631576|\n",
      "|          Italy|    38|       17| 0.4473684210526316|\n",
      "|    Switzerland|    51|       20|0.39215686274509803|\n",
      "|        Germany|   457|      146|0.31947483588621445|\n",
      "|         Sweden|    36|       10| 0.2777777777777778|\n",
      "|Channel Islands|    26|        7| 0.2692307692307692|\n",
      "|         Poland|    19|        5| 0.2631578947368421|\n",
      "|         Cyprus|    16|        4|               0.25|\n",
      "|       Portugal|    57|       13|0.22807017543859648|\n",
      "|           EIRE|   260|       59|0.22692307692307692|\n",
      "|        Belgium|    98|       21|0.21428571428571427|\n",
      "|      Australia|    57|       12|0.21052631578947367|\n",
      "| United Kingdom| 16649|     3208|0.19268424530001801|\n",
      "|         France|   389|       69|0.17737789203084833|\n",
      "|        Finland|    41|        7|0.17073170731707318|\n",
      "|          Spain|    90|       15|0.16666666666666666|\n",
      "|        Denmark|    18|        3|0.16666666666666666|\n",
      "|        Austria|    17|        2|0.11764705882352941|\n",
      "|         Norway|    36|        4| 0.1111111111111111|\n",
      "|    Netherlands|    95|        6|0.06315789473684211|\n",
      "+---------------+------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_ratio_df10 = return_ratio_df.filter(return_ratio_df[\"sumBuy\"] > 10)\n",
    "return_ratio_df10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ratio_df10.toPandas().to_csv('returnratio10.csv') #save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.  What is the relationship between average unit price and the quantity sold?\n",
    "Since the unit price of an item is constantly changing, we use the average to measure for each item and we use the sum of quantity to calculate quantity sold. We then group by stock code and sort sum of quantity sold either descendingly or ascendingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort 'sumQuan' in descending order\n",
      "+---------+-------------------+-------+\n",
      "|StockCode|       avgUnitPrice|sumQuan|\n",
      "+---------+-------------------+-------+\n",
      "|    84077|              0.284|  53215|\n",
      "|    22197| 1.0666666666666667|  48712|\n",
      "|   85099B|  2.098888888888889|  45066|\n",
      "|    84879| 1.5699999999999998|  35314|\n",
      "|   85123A|              3.355|  34204|\n",
      "|    21212|              0.655|  33409|\n",
      "|    23084|             2.4175|  27094|\n",
      "|    22492| 0.9433333333333334|  25880|\n",
      "|    22616|0.33875000000000005|  25321|\n",
      "|    21977|              0.655|  24163|\n",
      "|    17003|            7.48125|  22960|\n",
      "|    22178|              1.545|  21984|\n",
      "|    15036|              0.696|  21132|\n",
      "|    21915|             1.4375|  20912|\n",
      "|    22386| 2.1557142857142852|  19709|\n",
      "|    23203| 1.8439999999999999|  18926|\n",
      "|    84991|              0.655|  17539|\n",
      "|    20725|               2.41|  17345|\n",
      "|    22469| 1.7385714285714282|  16640|\n",
      "|   85099F| 2.1557142857142857|  16557|\n",
      "+---------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Sort 'sumQuan' in ascending order\n",
      "+---------+-----------------+-------+\n",
      "|StockCode|     avgUnitPrice|sumQuan|\n",
      "+---------+-----------------+-------+\n",
      "|    84347|             2.21|  -1460|\n",
      "|        D|74.07453333333335|  -1194|\n",
      "|    21645|             1.65|    -24|\n",
      "|     CRUK|       495.839375|    -16|\n",
      "|    21144|             0.38|    -12|\n",
      "|    22034|             0.42|     -9|\n",
      "|   79323W|             6.75|     -8|\n",
      "|    20703|             4.25|     -6|\n",
      "|    21412|             0.42|     -6|\n",
      "|    35400|             8.45|     -6|\n",
      "|    22769|            23.45|     -4|\n",
      "|   79323P|             6.75|     -4|\n",
      "|   84750A|             1.95|     -3|\n",
      "|    85042|             4.95|     -3|\n",
      "|    85063|            15.95|     -3|\n",
      "|    72815|             3.75|     -2|\n",
      "|    85068|             7.95|     -1|\n",
      "|   84856S|             3.81|     -1|\n",
      "|    79320|             4.95|     -1|\n",
      "|    85126|             6.75|     -1|\n",
      "|    20957|             1.45|     -1|\n",
      "|    35832|             2.95|     -1|\n",
      "|    84839|             5.55|     -1|\n",
      "|    85065|            12.75|     -1|\n",
      "|   85098B|             3.75|     -1|\n",
      "|    37503|            10.75|     -1|\n",
      "|   85023C|             2.55|     -1|\n",
      "|    21667|            14.95|      0|\n",
      "|    21655|             1.69|      0|\n",
      "|    85047|5.949999999999999|      0|\n",
      "|   90177E|             2.95|      0|\n",
      "|   90059A|             1.65|      0|\n",
      "|   90202A|             2.95|      0|\n",
      "|   90059D|             1.65|      0|\n",
      "|   90182C|             2.95|      0|\n",
      "|    90168|             2.95|      0|\n",
      "|    90118|             2.55|      0|\n",
      "|    23630|             2.99|      0|\n",
      "|   79323B|             6.75|      0|\n",
      "|    90169|             2.55|      0|\n",
      "|   90177A|             2.95|      0|\n",
      "|    23843|             2.08|      0|\n",
      "|    84227|             0.42|      1|\n",
      "|    90021|            11.95|      1|\n",
      "|    22146|             1.95|      1|\n",
      "|    20892|            12.75|      1|\n",
      "|    90055|             2.55|      1|\n",
      "|   90128B|             1.25|      1|\n",
      "|   85031B|             4.95|      1|\n",
      "|   90176A|              7.5|      1|\n",
      "+---------+-----------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sort 'sumQuan' in descending order\")\n",
    "unitp_salesdf = spark.sql(\"\"\"select StockCode,\n",
    "                                avg(distinct UnitPrice) as avgUnitPrice,\n",
    "                                sum(Quantity) as sumQuan\n",
    "                            from df1\n",
    "                            group by StockCode\"\"\")\n",
    "unitp_salesdf = unitp_salesdf.sort(F.col(\"sumQuan\").desc())\n",
    "unitp_salesdf.show()\n",
    "\n",
    "print(\"Sort 'sumQuan' in ascending order\")\n",
    "unitp_salesdf.sort(F.col(\"sumQuan\")).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see generally higher prices are related with less sales. Interestingly, we see many negative numbers for sum of quantity sold, which could mean that they were not bought within our analysis period (Dec 2010 - Dec 2011) but returned during this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitp_salesdf.toPandas().to_csv('unitpricesales.csv') #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
